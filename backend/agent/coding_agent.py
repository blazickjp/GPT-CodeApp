# import os
import io
import re
import json
import boto3
import instructor
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional, Callable
from database.my_codebase import MyCodebase

client = instructor.patch(OpenAI())
# GPT_MODEL = "gpt-3.5-turbo-0613"  # or any other chat model you want to use
GPT_MODEL = "gpt-4-1106-preview"  # or any other chat model you want to use
# GPT_MODEL = "anthropic"  # or any other chat model you want to use
MAX_TOKENS = 4000  # or any other number of tokens you want to use
TEMPERATURE = 0.75  # or any other temperature you want to use


class FunctionCall(BaseModel):
    name: Optional[str] = None
    arguments: str = ""


class Message(BaseModel):
    role: str
    content: str

    def to_dict(self):
        return {
            "role": self.role,
            "content": self.content,
        }


class NewMessage(BaseModel):
    role: str
    content: str
    file_path: Optional[str] = None
    line_number: Optional[int] = None

    def to_dict(self):
        return {
            "role": self.role,
            "content": self.content,
            "file_path": self.file_path,
            "line_number": self.line_number,
        }


class CodingAgent:
    """
    A class to represent a coding agent that uses OpenAI's GPT-3 model to generate code.

    Attributes:
        memory_manager (MemoryManager): Manages the memory of the agent.
        functions (Optional[List[dict]]): A list of functions that the agent can call.
        callables (Optional[List[Callable]]): A list of callable functions.
        GPT_MODEL (str): The GPT-3 model used by the agent.
        function_map (dict): A dictionary mapping function names to their callable objects.
    """

    def __init__(
        self,
        memory_manager,
        functions: Optional[List[dict]] = None,
        callables: Optional[List[Callable]] = None,
        codebase: Optional[MyCodebase] = None,
    ):
        """
        Constructs all the necessary attributes for the CodingAgent object.

        Args:
            memory_manager (MemoryManager): Manages the memory of the agent.
            functions (Optional[List[dict]]): A list of functions that the agent can call.
            callables (Optional[List[Callable]]): A list of callable functions.
        """
        self.memory_manager = memory_manager
        self.functions = functions
        self.callables = callables
        self.GPT_MODEL = GPT_MODEL
        self.codebase = codebase
        self.buff = io.BytesIO()
        self.read_pos = 0
        self.function_map = {}  # Initialize the function map here
        self.function_to_call = None
        if callables:
            self.function_map = {
                func.__name__: func for func in callables if func is not None
            }

    def query(self, input: str, command: Optional[str] = None) -> List[str]:
        """
        Queries the GPT-3 model with the given input and command.

        Args:
            input (str): The input text to be processed by the GPT-3 model.
            command (Optional[str]): The command to be executed by the agent.

        Returns:
            List[str]: The output generated by the GPT-3 model.
        """
        print(f"Input Text: {input}")
        self.memory_manager.add_message("user", input)

        message_history = [
            Message(**i).to_dict() for i in self.memory_manager.get_messages()
        ]

        keyword_args = {
            "model": self.GPT_MODEL,
            "messages": message_history,
            "max_tokens": MAX_TOKENS,
            "temperature": TEMPERATURE,
            "stream": True,
        }
        if self.functions:
            keyword_args["functions"] = self.functions
            keyword_args["function_call"] = "auto"

        # Override normal function calling when function_name is provided
        if command:
            print(f"Command: {command}")
            self.function_to_call = FunctionCall(name=command)
            if command not in self.function_map:
                raise ValueError(f"Function {command} not registered with Agent")

            # function_to_call = [self.function_map.get(command).openai_schema]
            keyword_args["function_call"] = {"name": command}
            if command == "Changes":
                # self.memory_manager.identity = (
                #     self.memory_manager.identity
                #     + "\nLine numbers have been added to the Current File to aid in your response. They are not part of the actual file."
                # )
                # self.set_files_in_prompt(include_line_numbers=True)
                # keyword_args["functions"] = [Changes.openai_schema]
                keyword_args["max_tokens"] = 2000

        # Call the model
        print(f"Calling model: {self.GPT_MODEL}")
        for i, chunk in enumerate(self.call_model_streaming(command, **keyword_args)):
            print(chunk)
            delta = chunk.choices[0].delta
            if "function_call" in delta:
                yield from self.process_function_call(delta, i)
            if self.should_stop_and_has_function(chunk):
                yield from self.execute_function()
            else:
                yield delta.content

    def process_function_call(self, delta, i):
        function_call = delta.function_call
        if function_call.name:
            self.function_to_call.name = function_call.name
        if function_call.arguments:
            if self.function_to_call.name == "Changes" and i == 0:
                yield "\n```json\n" + function_call.arguments
            else:
                self.function_to_call.arguments += function_call.arguments
                yield function_call.arguments

    def should_stop_and_has_function(self, delta):
        if self.function_to_call:
            return (
                delta.choices[0].finish_reason == "stop"
                and self.function_to_call.name is not None  # noqa 503
            )

        return delta.choices[0].finish_reason == "stop"

    def execute_function(self):
        print("Here")
        if not self.function_to_call:
            return
        if self.function_to_call.name == "Changes":
            yield "```\n\n"
        args = self.process_json(self.function_to_call.arguments)
        function_response = self.function_map[self.function_to_call.name](**args)
        if self.function_to_call.name == "Changes":
            diff = function_response.execute(self.codebase.directory)
            yield diff

    def process_json(self, args: str) -> str:
        """
        Process a JSON string, handling any triple-quoted strings within it.

        Args:
            args (str): The JSON string to process.

        Returns:
            str: The processed JSON string.
        """
        try:
            # Attempt to load the JSON string
            response = json.loads(args)
            return response
        except json.decoder.JSONDecodeError:
            # If there's a JSONDecodeError, it may be due to triple-quoted strings
            # Find all occurrences of triple-quoted strings
            triple_quoted_strings = re.findall(r"\"\"\"(.*?)\"\"\"", args, re.DOTALL)

            # For each occurrence, replace newlines and triple quotes
            for tqs in triple_quoted_strings:
                # Replace newlines and double quotes within the triple-quoted string
                fixed_string = tqs.replace("\n", "\\n").replace('"', '\\"')
                # Replace the original triple-quoted string with the fixed string
                response_str = args.replace(tqs, fixed_string)

            # Now replace the triple quotes with single quotes
            response_str = args.replace('"""', '"')

            return json.loads(response_str)

    def generate_anthropic_prompt(self) -> str:
        """
        Generates a prompt for the Gaive model.

        Args:
            input (str): The input text to be processed by the GPT-3 model.

        Returns:
            str: The generated prompt.
        """
        prompt = f"\n\nHuman: {self.memory_manager.system}\n\n"
        user_messages = 0
        for message in self.memory_manager.get_messages():
            if message["role"].lower() == "user":
                if user_messages == 0:
                    prompt += f"{message['content']}\n\n"
                    user_messages += 1
                else:
                    prompt += f"Human: {message['content']}\n\n"
            if message["role"].lower() == "assistant":
                prompt += f"Assistant: {message['content']}\n\n"

        return prompt + "Assistant:"

    def call_model_streaming(self, command: Optional[str] | None = None, **kwargs):
        print("Calling model streaming")
        print(kwargs.keys())
        self.read_pos = 0
        if self.GPT_MODEL != "anthropic":
            if command:
                print("Here")
                completion = client.chat.completions.create(**kwargs)
                yield Changes.from_streaming_response(completion)

            else:
                for chunk in client.chat.completions.create(**kwargs):
                    yield chunk

        if kwargs["model"] == "anthropic":
            print("Calling anthropic")
            try:
                sm_client = boto3.client("bedrock-runtime")
                resp = sm_client.invoke_model_with_response_stream(
                    accept="*/*",
                    contentType="application/json",
                    modelId="anthropic.claude-v2",
                    body=json.dumps(
                        {
                            "prompt": self.generate_anthropic_prompt(),
                            "max_tokens_to_sample": max(kwargs["max_tokens"], 2000),
                            "temperature": kwargs["temperature"],
                            # "stop_sequences": ["Human:"]
                        }
                    ),
                )
            except Exception as e:
                print(f"Error calling Anthropic Models: {e}")
                yield {
                    "choices": [
                        {
                            "finish_reason": "stop",
                            "delta": {"content": "Error: " + str(e)},
                        }
                    ]
                }

            while True:
                try:
                    chunk = next(iter((resp["body"])))
                    bytes_to_send = chunk["chunk"]["bytes"]
                    decoded_str = json.loads(bytes_to_send.decode("utf-8"))
                    content = decoded_str["completion"]
                    stop_reason = decoded_str["stop_reason"]
                    if stop_reason == "stop_sequence":
                        yield {
                            "choices": [
                                {"finish_reason": "stop", "delta": {"content": content}}
                            ]
                        }
                        break
                    else:
                        yield {
                            "choices": [
                                {"finish_reason": None, "delta": {"content": content}}
                            ]
                        }

                except StopIteration:
                    break

                except UnboundLocalError:
                    print("UnboundLocalError")
                    break
