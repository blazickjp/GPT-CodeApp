# import os
import re
import openai
import json
import os
import boto3

# import time

from typing import List, Optional, Callable
from pydantic import BaseModel
from database.my_codebase import MyCodebase

# from agent.agent_functions import Program, File

# GPT_MODEL = "gpt-3.5-turbo-0613"  # or any other chat model you want to use
GPT_MODEL = "gpt-4-1106-preview"  # or any other chat model you want to use
# GPT_MODEL = "anthropic"  # or any other chat model you want to use
MAX_TOKENS = 2000  # or any other number of tokens you want to use
TEMPERATURE = 0.75  # or any other temperature you want to use


class FunctionCall(BaseModel):
    name: Optional[str] = None
    arguments: str = ""


class Message(BaseModel):
    role: str
    content: str

    def to_dict(self):
        return {
            "role": self.role,
            "content": self.content,
        }


class CodingAgent:
    """
    A class to represent a coding agent that uses OpenAI's GPT-3 model to generate code.

    Attributes:
        memory_manager (MemoryManager): Manages the memory of the agent.
        functions (Optional[List[dict]]): A list of functions that the agent can call.
        callables (Optional[List[Callable]]): A list of callable functions.
        GPT_MODEL (str): The GPT-3 model used by the agent.
        function_map (dict): A dictionary mapping function names to their callable objects.
    """

    def __init__(
        self,
        memory_manager,
        functions: Optional[List[dict]] = None,
        callables: Optional[List[Callable]] = None,
        codebase: Optional[MyCodebase] = None,
    ):
        """
        Constructs all the necessary attributes for the CodingAgent object.

        Args:
            memory_manager (MemoryManager): Manages the memory of the agent.
            functions (Optional[List[dict]]): A list of functions that the agent can call.
            callables (Optional[List[Callable]]): A list of callable functions.
        """
        self.memory_manager = memory_manager
        self.functions = functions
        self.callables = callables
        self.GPT_MODEL = GPT_MODEL
        self.codebase = codebase
        self.read_pos = 0
        if callables:
            self.function_map = {
                func.__name__: func for func in callables if func is not None
            }

    def query(self, input: str, command: Optional[str] = None) -> List[str]:
        """
        Queries the GPT-3 model with the given input and command.

        Args:
            input (str): The input text to be processed by the GPT-3 model.
            command (Optional[str]): The command to be executed by the agent.

        Returns:
            List[str]: The output generated by the GPT-3 model.
        """
        print(f"Input Text: {input}")
        self.memory_manager.add_message("user", input)
        message_history = [
            Message(**i).to_dict() for i in self.memory_manager.get_messages()
        ]

        self.function_to_call = FunctionCall()

        keyword_args = {
            "model": self.GPT_MODEL,
            "messages": message_history,
            "max_tokens": MAX_TOKENS,
            "temperature": TEMPERATURE,
            "stream": True,
        }
        if self.functions:
            keyword_args["functions"] = self.functions
            keyword_args["function_call"] = "auto"

        # Override normal function calling when function_name is provided
        if command:
            if command not in self.function_map:
                raise ValueError(f"Function {command} not registered with Agent")

            keyword_args["functions"] = [self.function_map.get(command).openai_schema]
            keyword_args["function_call"] = {"name": command}

            if command == "Changes":
                # self.memory_manager.identity = (
                #     self.memory_manager.identity
                #     + "\nLine numbers have been added to the Current File to aid in your response. They are not part of the actual file."
                # )
                # self.set_files_in_prompt(include_line_numbers=True)
                keyword_args["model"] = "gpt-4-1106-preview"
        print(f"Calling model: {self.GPT_MODEL}")
        for i, chunk in enumerate(self.call_model_streaming(**keyword_args)):
            delta = chunk["choices"][0].get("delta", {})
            if "function_call" in delta:
                yield from self.process_function_call(delta, i)
            if self.should_stop_and_has_function(chunk):
                yield from self.execute_function()
            else:
                yield delta.get("content")

    def process_function_call(self, delta, i):
        function_call = delta["function_call"]
        if "name" in function_call:
            self.function_to_call.name = function_call["name"]
        if "arguments" in function_call:
            if self.function_to_call.name == "Changes" and i == 0:
                yield "\n```json\n" + function_call["arguments"]
            else:
                self.function_to_call.arguments += function_call["arguments"]
                yield function_call["arguments"]

    def should_stop_and_has_function(self, delta):
        return (
            delta["choices"][0]["finish_reason"] == "stop"
            and self.function_to_call.name  # noqa 503
        )

    def execute_function(self):
        if self.function_to_call.name == "Changes":
            yield "```\n\n"
        args = self.process_json(self.function_to_call.arguments)
        function_response = self.function_map[self.function_to_call.name](**args)
        if self.function_to_call.name == "Changes":
            diff = function_response.execute(self.codebase.directory)
            yield diff

    def process_json(self, args: str) -> str:
        """
        Process a JSON string, handling any triple-quoted strings within it.

        Args:
            args (str): The JSON string to process.

        Returns:
            str: The processed JSON string.
        """
        try:
            # Attempt to load the JSON string
            response = json.loads(args)
            return response
        except json.decoder.JSONDecodeError:
            # If there's a JSONDecodeError, it may be due to triple-quoted strings
            # Find all occurrences of triple-quoted strings
            triple_quoted_strings = re.findall(r"\"\"\"(.*?)\"\"\"", args, re.DOTALL)

            # For each occurrence, replace newlines and triple quotes
            for tqs in triple_quoted_strings:
                # Replace newlines and double quotes within the triple-quoted string
                fixed_string = tqs.replace("\n", "\\n").replace('"', '\\"')
                # Replace the original triple-quoted string with the fixed string
                response_str = args.replace(tqs, fixed_string)

            # Now replace the triple quotes with single quotes
            response_str = args.replace('"""', '"')

            return json.loads(response_str)

    def generate_anthropic_prompt(self) -> str:
        """
        Generates a prompt for the Gaive model.

        Args:
            input (str): The input text to be processed by the GPT-3 model.

        Returns:
            str: The generated prompt.
        """
        conversation_history = "The following is a portion of your conversation history with the human, truncated to save token space, inside the <conversation-history></conversation-history> XML tags.\n\n<conversation-history>\n"
        messages = self.memory_manager.get_messages()
        # Extract the last User messages
        print(messages)
        last_user_message = "\n\nThe most recent message from the human is tagged below in the <last-message></last-message> XML tags. Your response should ALWAYS adress this question or request from the human.\n<last-message>\n" + [message['content'] for message in messages if message["role"] == "user"][-1] + "\n</last-message>"


        for idx, message in enumerate(messages):
            if message["role"].lower() == "user":
                    conversation_history += f"Human: {message['content']}\n\n"
            if message["role"].lower() == "assistant":
                conversation_history += f"Assistant: {message['content']}\n\n"
        conversation_history += "\n</conversation-history>\n\n"
        
        if self.memory_manager.system_file_contents:
            file_context = "The human as loadedd the following files into context to help give you background related to the most recent request. They are contained in the <file-contents></file-contenxt> XML Tags.\n\n<file-contents>\n" + self.memory_manager.system_file_contents + "\n</file-contents>\n\n"
        else:
            file_context = ""
        if self.memory_manager.tree:
            tree = "The working directory of the human is always loaded into context. This information is good background when the human is working on the project, but this may not always be the case. Sometimes the human may ask questions not related to the current project <directory-tree></directory-tree> XML Tags\n<directory-tree>\n" + self.memory_manager.tree + "\n</directory-tree>\n\n"
        else:
            tree = ""
        
        if include_messages:
            prompt = "\n\nHuman: " + self.memory_manager.identity + tree + file_context + conversation_history
        else:
            prompt = "\n\nHuman: " + self.memory_manager.identity + tree + file_context

        return prompt + last_user_message + "\n\nPlease respond accordingly.\n\nAssistant:"

    def call_model_streaming(self, **kwargs):
        self.read_pos = 0
        if self.GPT_MODEL == "gpt-4-1106-preview" or self.GPT_MODEL == "gpt-3.5-turbo":
            for chunk in openai.ChatCompletion.create(**kwargs):
                yield chunk

        if self.GPT_MODEL == "anthropic":
            print("Calling anthropic")
            try:
                sm_client = boto3.client("bedrock-runtime")
                resp = sm_client.invoke_model_with_response_stream(
                    accept="*/*",
                    contentType="application/json",
                    modelId="anthropic.claude-v2",
                    body=json.dumps(
                        {
                            "prompt": self.generate_anthropic_prompt(),
                            "max_tokens_to_sample": max(kwargs["max_tokens"], 2000),
                            "temperature": kwargs["temperature"],
                            # "stop_sequences": ["Human:"]
                        }
                    ),
                )
            except Exception as e:
                print(f"Error calling Anthropic Models: {e}")
                yield {
                    "choices": [
                        {
                            "finish_reason": "stop",
                            "delta": {"content": "Error: " + str(e)},
                        }
                    ]
                }

            while True:
                try:
                    chunk = next(iter((resp["body"])))
                    bytes_to_send = chunk["chunk"]["bytes"]
                    decoded_str = json.loads(bytes_to_send.decode("utf-8"))
                    content = decoded_str["completion"]
                    stop_reason = decoded_str["stop_reason"]
                    if stop_reason == "stop_sequence":
                        yield {
                            "choices": [
                                {"finish_reason": "stop", "delta": {"content": content}}
                            ]
                        }
                        break
                    else:
                        yield {
                            "choices": [
                                {"finish_reason": None, "delta": {"content": content}}
                            ]
                        }

                except StopIteration:
                    break

                except UnboundLocalError:
                    print("UnboundLocalError")
                    break
