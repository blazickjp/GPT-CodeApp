# import os
import openai
import json
from typing import List, Optional
from pydantic import BaseModel

# from agent.agent_functions import Program, File

GPT_MODEL = "gpt-3.5-turbo-0613"  # or any other chat model you want to use
MAX_TOKENS = 1000  # or any other number of tokens you want to use
TEMPERATURE = 0.2  # or any other temperature you want to use


class FunctionCall(BaseModel):
    name: Optional[str] = None
    arguments: str = ""


class Message(BaseModel):
    role: str
    content: str

    def to_dict(self):
        return {
            "role": self.role,
            "content": self.content,
        }


class CodingAgent:
    def __init__(self, memory_manager, functions=None, callables=[None]):
        """
        Initializes a CodingAgent object.

        Args:
            memory_manager (MemoryManager): An instance of MemoryManager class for managing conversation history.
            functions (list, optional): A list of function objects that can be called by the agent. Defaults to None.
        """
        self.memory_manager = memory_manager
        self.functions = functions
        self.GPT_MODEL = GPT_MODEL
        if callables:
            self.function_map = {
                func.__name__: func for func in callables if func is not None
            }  # Create a map of function names to functions

    def query(self, input_text: str) -> List[str]:
        """
        Conducts a conversation with the agent by providing an input text.

        Args:
            input_text (str): The user's input text.
        Returns:
            str: The output text generated by the agent.
        """
        print(f"Input Text: {input_text}")
        self.memory_manager.add_message("user", input_text)
        message_history = [
            Message(**i).to_dict() for i in self.memory_manager.get_messages()
        ]

        keyword_args = {
            "model": self.GPT_MODEL,
            "messages": message_history,
            "max_tokens": MAX_TOKENS,
            "temperature": TEMPERATURE,
            "stream": True,
        }
        if self.functions:
            keyword_args["functions"] = self.functions
            keyword_args["function_call"] = "auto"
            func_call = FunctionCall()

        for chunk in openai.ChatCompletion.create(**keyword_args):
            delta = chunk["choices"][0].get("delta", {})
            if "function_call" in delta:
                if "name" in delta.function_call:
                    func_call.name = delta.function_call["name"]
                if "arguments" in delta.function_call:
                    func_call.arguments += delta.function_call["arguments"]
            if chunk.choices[0].finish_reason == "function_call":
                print(f"Func Call: {func_call.name}")
                function_response = Message(
                    role="assistant",
                    content=json.dumps(
                        obj=self.function_map[func_call.name](func_call.arguments)
                    ),
                )
                message_history.append(function_response.to_dict())
                for chunk in openai.ChatCompletion.create(
                    model=self.GPT_MODEL,
                    messages=message_history,
                    max_tokens=MAX_TOKENS,
                    temperature=TEMPERATURE,
                    stream=True,
                ):
                    content = chunk["choices"][0].get("delta", {}).get("content")
                    yield content
            else:
                yield delta.get("content")
