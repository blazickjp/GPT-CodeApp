# import os
import re
import openai
import json
import os
from typing import Any, List, Optional, Callable
from pydantic import BaseModel
from database.my_codebase import MyCodebase

# from agent.agent_functions import Program, File

# GPT_MODEL = "gpt-3.5-turbo-0613"  # or any other chat model you want to use
GPT_MODEL = "gpt-4"  # or any other chat model you want to use
MAX_TOKENS = 1000  # or any other number of tokens you want to use
TEMPERATURE = 0.2  # or any other temperature you want to use


class FunctionCall(BaseModel):
    name: Optional[str] = None
    arguments: str = ""


class Message(BaseModel):
    role: str
    content: str

    def to_dict(self):
        return {
            "role": self.role,
            "content": self.content,
        }


class CodingAgent:
    """
    A class to represent a coding agent that uses OpenAI's GPT-3 model to generate code.

    Attributes:
        memory_manager (MemoryManager): Manages the memory of the agent.
        functions (Optional[List[dict]]): A list of functions that the agent can call.
        callables (Optional[List[Callable]]): A list of callable functions.
        GPT_MODEL (str): The GPT-3 model used by the agent.
        function_map (dict): A dictionary mapping function names to their callable objects.
    """

    def __init__(
        self,
        memory_manager,
        functions: Optional[List[dict]] = None,
        callables: Optional[List[Callable]] = None,
        codebase: Optional[MyCodebase] = None,
    ):
        """
        Constructs all the necessary attributes for the CodingAgent object.

        Args:
            memory_manager (MemoryManager): Manages the memory of the agent.
            functions (Optional[List[dict]]): A list of functions that the agent can call.
            callables (Optional[List[Callable]]): A list of callable functions.
        """
        self.memory_manager = memory_manager
        self.functions = functions
        self.callables = callables
        self.GPT_MODEL = GPT_MODEL
        self.codebase = codebase
        if callables:
            self.function_map = {
                func.__name__: func for func in callables if func is not None
            }
        self.files_in_prompt: List[str] = []

    def query(self, input: str, command: Optional[str] = None) -> List[str]:
        """
        Queries the GPT-3 model with the given input and command.

        Args:
            input (str): The input text to be processed by the GPT-3 model.
            command (Optional[str]): The command to be executed by the agent.

        Returns:
            List[str]: The output generated by the GPT-3 model.
        """
        print(f"Input Text: {input}")
        self.memory_manager.add_message("user", input)
        message_history = [
            Message(**i).to_dict() for i in self.memory_manager.get_messages()
        ]
        function_to_call = FunctionCall()

        keyword_args = {
            "model": self.GPT_MODEL,
            "messages": message_history,
            "max_tokens": MAX_TOKENS,
            "temperature": TEMPERATURE,
            "stream": True,
        }
        if self.functions:
            keyword_args["functions"] = self.functions
            keyword_args["function_call"] = "auto"

        # Override normal function calling when function_name is provided
        if command:
            if command not in self.function_map:
                raise ValueError(f"Function {command} not registered with Agent")

            keyword_args["functions"] = [self.function_map.get(command).openai_schema]
            keyword_args["function_call"] = {"name": command}

            if command == "Changes":
                # self.memory_manager.identity = (
                #     self.memory_manager.identity
                #     + "\nLine numbers have been added to the Current File to aid in your response. They are not part of the actual file."
                # )
                # self.set_files_in_prompt(include_line_numbers=True)
                keyword_args["model"] = "gpt-4"

        for chunk in openai.ChatCompletion.create(**keyword_args):
            delta = chunk["choices"][0].get("delta", {})
            if "function_call" in delta:
                if "name" in delta.function_call:
                    function_to_call.name = delta.function_call["name"]
                if "arguments" in delta.function_call:
                    function_to_call.arguments += delta.function_call["arguments"]
                    yield delta.function_call["arguments"]
            if chunk.choices[0].finish_reason == "stop" and function_to_call.name:
                print(
                    f"\n\nFunc Call: {function_to_call.name}\n\n{function_to_call.arguments}"
                )
                args = self.process_json(function_to_call.arguments)
                function_response = self.function_map[function_to_call.name](**args)
                print(f"Func Response: {json.dumps(function_response.to_dict())}")
                if function_to_call.name == "Changes":
                    diff = function_response.execute()
                    # Show the diff back to the user
                    yield diff

                function_message = {
                    "role": "function",
                    "name": function_to_call.name,
                    "content": diff,
                }

                message_history.append(function_message)
                for chunk in openai.ChatCompletion.create(
                    model=self.GPT_MODEL,
                    messages=message_history,
                    max_tokens=MAX_TOKENS,
                    temperature=TEMPERATURE,
                    stream=True,
                ):
                    content = chunk["choices"][0].get("delta", {}).get("content")
                    yield content
            else:
                yield delta.get("content")

    def set_files_in_prompt(self, include_line_numbers: Optional[bool] = None) -> None:
        """
        Sets the files in the prompt.

        Args:
            files (List[File]): A list of files to be set in the prompt.
        """
        file_contents = self.codebase.get_file_contents()
        files = [
            os.path.join(self.codebase.directory, f.lstrip("/"))
            for f in self.files_in_prompt
        ]
        content = ""
        for k, v in file_contents.items():
            if k in files and include_line_numbers:
                v = self.add_line_numbers_to_content(v)
                content += f"{k}:\n{v}\n\n"
            elif k in files:
                content += f"{k}:\n{v}\n\n"

        self.memory_manager.system_file_contents = content
        self.memory_manager.set_system()
        return

    def add_line_numbers_to_content(self, content: str) -> str:
        """
        Adds line numbers to the given content.

        Args:
            content (str): The content to add line numbers to.

        Returns:
            str: The content with line numbers added.
        """
        lines = content.split("\n")
        for i in range(len(lines)):
            lines[i] = f"{i+1} {lines[i]}"
        return "\n".join(lines)

    def process_json(self, args):
        try:
            response = json.loads(args)
            return response
        except json.decoder.JSONDecodeError:
            # Find all occurrences of triple-quoted strings
            triple_quoted_strings = re.findall(r"\"\"\"(.*?)\"\"\"", args, re.DOTALL)

            # For each occurrence, replace newlines and triple quotes
            for tqs in triple_quoted_strings:
                fixed_string = tqs.replace("\n", "\\n").replace('"', '\\"')
                response_str = args.replace(tqs, fixed_string)

            # Now replace the triple quotes with single quotes
            response_str = args.replace('"""', '"')

            return response_str
